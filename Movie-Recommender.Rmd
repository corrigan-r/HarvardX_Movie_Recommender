---
title: "Movie Recommendation System"
author: "R Corrigan"
date: "2021 July 5"
output:
  pdf_document: default
  html_document: default
subtitle: HarvardX Capstone
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)

library(caret)
library(dplyr)
library(ggplot2)
library(lubridate)
library(tidyr)
library(knitr)
library(kableExtra)

load("rmd_objects.RData")
```

# Overview

This report is a deliverable for the HavardX Capstone course, which is the ninth and final course of the HarvardX Data Science Professional Certificate Program. The challenge of the Capstone project is to create a movie selection system using the MovieLens dataset. The student is provided with a data set of movie ratings and is tasked with building an algorithm that predicts ratings that viewers give to movies. Once developed, the algorithm's utility is assessed by quantifying the closeness of predicted values to actual values on a separate validation set using the root mean squared error (RMSE) metric:

$$RMSE = \sqrt{\frac{1}{N} \sum(\hat{y}_{u,i}-y_{u,i})^2} $$
where $N$ is the number of predictions, $\hat{y}_{u,i}$ is the predicted rating by user $u$ of movie $i$ and $y_{u,i}$ is the actual rating found in the validation set. Based on the grading rubric provided for the Capstone project, the RMSE component of the final grade achieves full marks if it is less than 0.86490. Thus the main technical criterion is to produce a model that predicts users' movie ratings with a RMSE of less than 0.86490.

The MovieLens 10M dataset, consisting of 10 million observations and available from <http://grouplens.org>, is used for the Capstone. HarvardX provided preliminary R code to refine the MovieLens dataset and partition it into two separate datasets: 1) an *edx* set, which is used to develop the prediction algorithm; 2) a *validation* set, which is used for the final test of the algorithm and calculation of the RMSE. The validation set is a partition of 10% percent of the MovieLens 10M dataset, and the remainder comprises the edx set. Thus, the edx set contains approximately 9 million observations (90% of the full dataset) for development of the algorithm. 
The edx data set  is organized in a data frame. The first 10 rows are represented in the table below:

```{r edx table}
kable(head(edx,10), col.names = c("User ID", "Movie ID", "Rating", "Time Stamp", "Title", "Genres"), align = "llllll", format = "latex", booktabs=TRUE) %>% kable_styling(latex_options = c("hold_position", "scale_down"))
```

As shown, each observation includes a user, a movie, a rating, a time stamp, and applicable genres.

The dataset includes `r n_distinct(edx$movieId)` unique movies and `r n_distinct(edx$userId)` unique users. The average rating is `r round(mean(edx$rating), 2)` with a standard deviation is `r round(sd(edx$rating), 2)`.


# Methods

To train a prediction algorithm, the edx dataset was further partitioned with 90% assigned to a train set and the other 10% retained as a test set. The rating prediction algorithm leverages the effects of features identified in the train set and uses regularization where appropriate.  The predictors selected for the model are the movie, the user, the movie genre, and the time of rating. The potential prediction value of each of these features is discussed below. 

## Predictors

### Movie

Some movies are better than others and will garner higher ratings. In evidence, the three highest rated movies with more than 1000 ratings -- *The Godfather*, *The Shawshank Redemption*, and *The Usual Suspects* -- each have average ratings close to 4.5. Conversely, the bottom three movies with more than 1000 ratings have average ratings close to 1.7.

### User

Users were found to have different biases, some having propensities to rate high and others to rate low. Even among users who have rated a large number of movies (i.e., more than 500), their average ratings vary substantially, as shown in the following plot.

```{r avg user ratings}
train_set %>% group_by(userId) %>% filter(n()>500) %>% 
  summarize(user = "Users with more than 500 ratings", avg_rating = mean(rating)) %>% 
  ggplot(aes(user, avg_rating)) + geom_boxplot() + 
  geom_jitter(width = 0.03, alpha = .1) + xlab("") + ylab("Average rating (by user)")

mean_avg <- train_set %>% group_by(userId) %>% 
  filter(n()>500) %>% summarize(average = mean(rating))
```

The plot reveals a clear user effect. While it can be seen that the average of the users' average ratings is close to `r round(mean(mean_avg$average), 1)`, some users are giving ratings of mostly 1 or 1.5 while others are giving ratings of mostly 4 or 4.5. 

### Genre

Most movies have multiple genre labels. For example, the movie *Outbreak* is labeled with four genres: action, drama, sci-fi, and thriller. In total, there are over 700 different combinations of genre labels for the movies in the edx set. For simplicity, the plot below includes only movies that are labeled with one genre to illustrate the effect of genre on rating.

```{r plot_genre_average_ratings, echo=FALSE}
edx %>% filter(genres %in% c("Action", "Adventure", "Animation", "Children", 
  "Comedy", "Drama", "Fantasy", "IMAX", "Musical", "Sci-Fi", "Thriller", "War")) %>%  
  group_by(genres) %>% summarize(avg_rating = mean(rating)) %>% 
  mutate(genres = reorder(genres, avg_rating)) %>% ggplot(aes(genres, avg_rating)) + 
  geom_col()+ theme(axis.text.x = element_text(angle = 45, hjust = 1)) + 
  labs(x = "Genre", y = "Average rating")

 genre_count <- edx %>% group_by(genres) %>% 
   summarize(count = n()) %>% slice_min(count, n = 15)
```


Drama and war movies, for example, are rated an average of about `r round(mean(edx$rating[which(edx$genres == "War")]) - mean(edx$rating[which(edx$genres == "Children")]), digits = 1)` greater than IMAX and children movies. Thus, genre effects are a useful component of the model. 

### Time

A scatter plot of ratings against time reveals a subtle time dependence of ratings.

 ```{r time series plot, echo=FALSE}
 train_set %>% mutate(week = round_date(as_datetime(timestamp), unit = "week")) %>%
   group_by(week) %>%
   summarize(rating = mean(rating)) %>%
   ggplot(aes(week, rating)) +
   geom_point() +
   geom_smooth() + labs(x = "Year", y = "Average rating (by week)")
 ```

```{r rating count by quarter, echo = FALSE}
rating_count_quarter <- mutate(train_set, date = as_datetime(timestamp))%>% 
  mutate(quarter = round_date(date, "quarter")) %>% group_by(quarter) %>% 
  summarize(n = n())%>% slice_min(order_by = n, n = 3)
```
 

We can see that movie ratings decline steadily until about 2005, after which they begin to ascend; modeling time effects can improve rating predictions. Based on visual assessment of the plot, estimating time effects at the resolution of quarter years (i.e. averaging ratings over bins of 3 months) is sensible. 

## Regularization

Regularization was employed to reduce the potential of large effects from small sample sizes. Some movies received few ratings and movie effects calculated for these movies are therefore prone to greater variance. It is desirable to assign less weight to these effects compared to effects derived from larger sample sizes. The plot below shows there are thousands of movies with 15 or less ratings.

```{r plot number of ratings by movie}
edx %>% group_by(movieId) %>% mutate(count = n()) %>% ungroup() %>% 
  filter(count < 25) %>% group_by(count) %>% summarize(nratings = n()) %>% 
  ggplot(aes(count, nratings, fill = count)) + 
  geom_bar(stat = "identity") + 
  labs(x = "Number of ratings", y = "Number of movies") + 
  scale_fill_gradient2(low = "red3", high = "forestgreen", mid = "grey79", 
                       midpoint = 16, breaks=c(2,23), 
                       labels=c("More need for regularization", 
                                "Less need for regularization")) + 
  theme(legend.title = element_blank()) 

low_genre_ratings <- edx %>% group_by(genres) %>% 
  summarize(count = n()) %>% filter(count <= 15) %>% nrow()

low_user_ratings <- edx %>% group_by(userId) %>% 
  summarize(count = n()) %>% filter(count <= 15) %>%  nrow()
```

There are also `r low_genre_ratings` genre combinations and `r low_user_ratings` users associated with 15 or fewer ratings. With respect to time effects, stratifying the movie data by quarter-years results in no quarter with less than `r min(rating_count_quarter[-1,]$n)` ratings, except for 1995 for which there are only two ratings. Therefore, the two ratings from 1995 are excluded from analysis, and regularization is not necessary for the time effect term of the model.

Based on the abovementioned, regularization penalty parameters are incorporated for movie, user, and genre effects. The regularization parameters, \boldmath$\lambda$\unboldmath, were optimized by training the prediction model with the parameters set to integer values ranging from `r min(lambdas)` to `r max(lambdas)`, and retaining the values that resulted in the lowest RMSE on the test set. Details of regularization calculations are provided in the following section on the model.

## Model

Based on the evident rating effects from the features of movie, user, genre, and time, the following model for movie ratings is assumed: $$Y_{m,u,g,t} = \mu + b_{m} + b_{u} + b_{g} + b_{t} + \epsilon_{m,u,g,t}$$ where $Y_{m,u,g,t}$ is the true rating for movie $i$ by user $u$ at time $t$; $\mu$ is the average rating for movie $i$; $b_{m}$, $b_{u}$, $b_{g}$, and $b_{t}$ are the rating effects of movie, user, genre, and time, respectively; and $\epsilon_{m,u,g,t}$ is the combined error.

The movie effect, $b_{m}$ was first calculated assuming a model that incorporates only the movie effect $$Y_{m} = \mu + b_{m} + \epsilon_{m}$$ and estimating the movie effect as $$\hat{b}_{m}=\frac{1}{n_{m} + \lambda_{m}}\sum(Y_{m}-\hat{\mu})$$ where $\lambda_{m}$ is the regularization parameter for the movie effect and $n_{m}$ is the number of distinct movies. Next, $b_{u}$ was calculated assuming a model incorporating the movie and user effects $$Y_{m,u} = \mu + b_{m} + b_{u} + \epsilon_{m,u}$$ and calculating the user effect as $$\hat{b}_{u}=\frac{1}{n_{u} + \lambda_{u}}\sum(Y_{m}-\hat{\mu}-b_{m}).$$ This method was continued for estimating the remaining effects, $b_{g}$ and $b_{t}$. 
  
The model was then used to predict all movie ratings in the validation set, and the RMSE or these predictions was calculated to assess the utility of the prediction system.


# Results

The optimal values for the regularization penalties, $\lambda_m$, $\lambda_u$, and $\lambda_g$, were found to be `r lambda_m_optimal`, `r lambda_u_optimal`, and `r lambda_g_optimal`, respectively. These were optimal in the sense that they provided the smallest RMSE on the test set, which was `r round(rmse_test, 5)`. With these parameter values incorporated in the model, and with the effects calculated for all movies, users, genres, and times (quarters), movie ratings were predicted for the validation set. The predicted ratings resulted in a RMSE of `r round(rmse_validation, 5)`, thus meeting the highest technical  criterion of a RMSE of less than 0.86490. 


# Conclusion

A movie recommendation system was developed using the Movie Lens 10M data set. The dataset was partitioned so that 90% percent used for training and testing of the prediction algorithm, while the other 10% was sequestered as the validation set for assessing the algorithm's performance. The developed algorithm predicts ratings of movies by using a simple model that incorporates estimated effects from four features in the data set: movie, user, genre, and time of rating. Regularization was applied in the calculation of the movie, user, and genre effects due to the small number of observations for some instances of these features. The final test of the algorithm on the validation set produced a RMSE of `r round(rmse_validation, 5)` for the predicted ratings.